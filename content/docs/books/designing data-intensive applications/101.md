---
weight: 1
title: "01: Reliable, Scalable, and Maintainable Applications"
bookHidden: false
---

# Reliable, Scalable, and Maintainable Applications
---

An application has to meet various requirements in order to be useful. Some nonfunctional requirements: security, reliability, compliance, scalability, compatibility, and maintainability.

Data-intensive applications - amount of data, the complexity of data, and the speed at which it is changing. In these days those are the most common applications.

Compute-intensive applications - CPU power is a limiting factor.

Applications usually need to:
- Store data (databases)
- Speed up reads (caches)
- Search data (search indexes)
- Send a message to another process asynchronously (stream processing)
- Periodically crunch data (batch processing)

The border is blur, because there are datastores that are also used as message
queues (Redis), and there are message queues with database-like durability guarantees
(Apache Kafka).


## Reliability

Reliability means making systems work correctly, even when faults occur.

Faults in a system can be handled with fault-tolerant or resilient systems. 

A fault is not the same as a failure, first is about one components, the other about the whole system. Prevent faults from causing failures.

### Hardware Faults

Hard disk crashes, faulty RAM, blackouts in the power grid, and accidental disconnection of network cables. Hard disks have an MTTF of 10 to 50 years.

To decrease the failure rate, redundancy is typically added to hardware components.


### Software Errors
Software errors are systematic faults that can cause many system failures.
Unlike hardware faults, these errors are often correlated across nodes.

Measures to reduce the impact of software errors include:
- thorough testing,
- process isolation,
- allowing processes to crash and restart,
- careful consideration of assumptions and interactions in the system.


### Human Errors
Human errors are a leading cause of outages, with configuration errors being the primary culprit. Only a small percentage of outages are due to hardware faults.

Prevention:
- Design systems to minimize opportunities for error, including well-designed abstractions, APIs, and admin interfaces.
- Provide fully-featured non-production sandbox environments for experimentation.
- Test thoroughly at all levels, from unit tests to whole-system integration tests and manual - tests.
- Allow quick and easy recovery from errors.
- Set up detailed and clear monitoring of system behavior.
- Implement good management practices.
- Provide training to operators.

## Scalability

Scalability is the ability to maintain good performance as load increases by adding processing capacity.

Architect needs to describe the right parameters to measure. Load parameters are used to describe system load and may include requests per second to a web server, read-to-write ratios in a database, active users in a chat room, cache hit rates, or other relevant factors. Once you have described the load on your system, you can investigate what happens
when the load increases.

In batch processing systems like Hadoop, throughput is essential, whereas online systems prioritize response time, specifically the time between a client sending a request and receiving a response.

Service level objectives and agreements (SLOs and SLAs) often specify response time metrics, which enable customers to demand a refund if not met. Response time includes the actual time to process the request, network delays, and queuing delays, whereas latency refers to the time a request waits to be handled.

DEF
Percentiles are commonly used to measure response time, with high percentiles (95th, 99th, and 99.9th) being important for understanding tail latencies.

IMPORTANT
Response time and latency are not synonyms. Response time includes network and queuing delays, in addition to the actual service time, whereas latency is the time a request spends waiting to be handled.

### Approaches for Coping with Load

People often talk of a dichotomy between scaling up (vertical scaling, moving to a
more powerful machine) and scaling out (horizontal scaling, distributing the load
across multiple smaller machines). Distributing load across multiple machines is also
known as a shared-nothing architecture

using several fairly powerful
machines can still be simpler and cheaper than a large number of small virtual
machines.

Some systems are elastic, meaning that they can automatically add computing resources
when they detect a load increase, whereas other systems are scaled manually

For this reason, common wisdom until recently
was to keep your database on a single node (scale up) until scaling cost or highavailability
requirements forced you to make it distributed.


The architecture of systems that operate at large scale is usually highly specific to the
application—there is no such thing as a generic, one-size-fits-all scalable architecture
(informally known as magic scaling sauce).


An architecture that scales well for a particular application is built around assumptions
of which operations will be common and which will be rare—the load parameters.


## Maintainability

Maintainability has many facets, but in essence it’s about making life better for the
engineering and operations teams who need to work with the system

three design principles for software
systems:
Operability
Make it easy for operations teams to keep the system running smoothly.
Simplicity
Make it easy for new engineers to understand the system, by removing as much
complexity as possible from the system. (Note this is not the same as simplicity
of the user interface.)
Evolvability
Make it easy for engineers to make changes to the system in the future, adapting
it for unanticipated use cases as requirements change. Also known as extensibility,
modifiability, or plasticity.

### Operability
good
operations team typically is responsible for the following, and more [29]:
• Monitoring the health of the system and quickly restoring service if it goes into a
bad state
• Tracking down the cause of problems, such as system failures or degraded performance
• Keeping software and platforms up to date, including security patches
• Keeping tabs on how different systems affect each other
Maintaining the security of the system

Good operability means making routine tasks easy, allowing the operations team to
focus their efforts on high-value activities.


### Simplicity

A software project mired in complexity is sometimes
described as a big ball of mud

symptoms of complexity: explosion of the state space, tight
coupling of modules, tangled dependencies, inconsistent naming and terminology,
hacks aimed at solving performance problems, special-casing to work around issues
elsewhere, and many more

One of the best tools we have for removing accidental complexity is abstraction. A
good abstraction can hide a great deal of implementation detail behind a clean,
simple-to-understand façade.

For example, high-level programming languages are abstractions that hide machine
code, CPU registers, and syscalls. SQL is an abstraction that hides complex on-disk
and in-memory data structures, concurrent requests from other clients, and inconsistencies
after crashes.


### Evolvability


In terms of organizational processes, Agile working patterns provide a framework for
adapting to change. The Agile community has also developed technical tools and patterns
that are helpful when developing software in a frequently changing environment,
such as test-driven development (TDD) and refactoring

The ease with which you can modify a data system, and adapt it to changing requirements,
is closely linked to its simplicity and its abstractions:

word to refer to agility on a data system
level: evolvability